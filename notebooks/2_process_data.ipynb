{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4fe24f",
   "metadata": {},
   "source": [
    "# Data processing & feature extraction\n",
    "\n",
    "This notebook does the following:\n",
    "\n",
    "1. Opens file index to retrieve data file paths for all participants\n",
    "2. Selects participants for processing (manual input of participant IDs for inclusion)\n",
    "3. Generates a CSV file for each included participant that contains downsampled, synchronised, filtered and normalised data\n",
    "4. Plots overview of all signals for both conditions for each participant\n",
    "5. Extracts and saves entire condition (air vs co2) and time windowed features for each participant\n",
    "6. Merges extracted features from all participants into one file\n",
    "\n",
    "Input: PARTICIPANTS_TO_PROCESS\n",
    "\n",
    "Output: \n",
    "\n",
    "1. 'temp/synced_participant_data/' - synchronised, downsampled, normalised and filtered data for each participant\n",
    "2. 'segment_features/' - extracted features for each participant for entire segment (co2 and air separately)\n",
    "3. 'windowed_features/' - extracted features for each participant for each time window\n",
    "4. 'segment_features.csv' - merged segment features from all participants\n",
    "5. 'windowed_features.csv' - merged time windowed features from all participants\n",
    "\n",
    "* Due to a bug in latest versions of NeuroKit2, version 0.2.5 must be installed (https://github.com/neuropsychology/NeuroKit/issues/961)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e77eaa-49bb-4a64-9ef9-81a4e3b0045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils.constants import AirFiles, CO2Files\n",
    "from utils.timestamps import read_unix\n",
    "from classes.Participant import Participant\n",
    "from classes.DataHandler import DataHandler\n",
    "from utils.plots import Plots\n",
    "\n",
    "SYNCED_DATA_DIRECTORY = os.path.join(os.getcwd(), 'temp', 'synced_participant_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1589fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "if('notebooks' in os.getcwd()):\n",
    "    os.chdir('..')\n",
    "import json\n",
    "import pandas as pd\n",
    "from utils.constants import AirFiles, CO2Files, DATA_COLUMNS, FREQUENCIES\n",
    "from utils.load_data import load_data_with_event_matching\n",
    "from utils.timestamps import read_unix, read_j2000, j2000_to_unix, generate_biopac_unix_timestamps\n",
    "from classes.Participant import Participant\n",
    "from classes.DataHandler import DataHandler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.plots import Plots\n",
    "from utils.normalisation import eye_tracking as normalise_pupil_size\n",
    "import math\n",
    "\n",
    "SYNCED_DATA_DIRECTORY = os.path.join(os.getcwd(), 'temp', 'synced_participant_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file index JSON for reading\n",
    "file_index = pd.read_json(os.path.join(os.getcwd(), 'temp/file_index.json'))\n",
    "file_index = file_index.sort_index()\n",
    "file_index_ids = file_index.index\n",
    "print('File index contains entries for ' + str(len(file_index_ids)) + ' participants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07718507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participants to process\n",
    "PARTICIPANTS_TO_PROCESS = [2,7,9,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26,27,28,\n",
    "                           29,32,33,34,35,36,37,38,43,44,45,46,47,48,49,51,52,53,54,\n",
    "                           55,57,59,60,61,62,63]\n",
    "PARTICIPANTS_TO_PROCESS = [id - 1 for id in PARTICIPANTS_TO_PROCESS]\n",
    "\n",
    "print('Selected ' + str(len(PARTICIPANTS_TO_PROCESS)) + ' participants to be processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c97258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that generates downsampled, synced files for every participant\n",
    "for participant_id in PARTICIPANTS_TO_PROCESS:\n",
    "    participant_to_retrieve = participant_id\n",
    "    participant_file_index = file_index[0][participant_to_retrieve]\n",
    "    participant = Participant(participant_file_index['id'])\n",
    "    \n",
    "    synced_participant_file = os.path.join(SYNCED_DATA_DIRECTORY, str(participant.id)) + '.csv'\n",
    "    if(os.path.exists(synced_participant_file)):\n",
    "        print('Synced file for participant: ' + str(participant.id) + ' found. Loading existing file.')\n",
    "        participant.set_synced_data(pd.read_csv(synced_participant_file))\n",
    "    else:\n",
    "        print('Generating new synced participant file for participant: ' + str(participant.id))\n",
    "        \n",
    "        # AIR\n",
    "        if(participant_file_index[AirFiles.MASK.value] is None):\n",
    "            print('Air mask file missing')\n",
    "        else:\n",
    "            print('Loading Air condition Data')\n",
    "            air_mask_file = participant_file_index[AirFiles.MASK.value]\n",
    "            air_event_file = participant_file_index[AirFiles.EVENT.value]\n",
    "            air_eyetracking_file = participant_file_index[AirFiles.EYE.value]\n",
    "            air_biopac_file = participant_file_index[AirFiles.BIOPAC.value]\n",
    "            air_biopac_start_unix = participant_file_index[AirFiles.BIOPAC_UNIX_START_TIME.value]\n",
    "            print(read_unix(air_biopac_start_unix))\n",
    "\n",
    "            # Load mask data\n",
    "            participant.set_air_mask_data(DataHandler.load_mask_data(air_mask_file, air_event_file, participant.id))\n",
    "            # Load eye tracking data\n",
    "            participant.set_air_eye_data(DataHandler.load_eyetracking_data(air_eyetracking_file, participant.id, 'air'))  \n",
    "            # Load biopac data\n",
    "            participant.set_air_biopac_data(DataHandler.load_biopac_data(air_biopac_file, air_biopac_start_unix, participant.id))  \n",
    "            # Sync eye tracking and biopac data\n",
    "            participant.set_air_synced_data(DataHandler.sync_signal_data(participant.air_mask_data, participant.air_eye_data, participant.air_biopac_data, air_biopac_start_unix))\n",
    "\n",
    "        #CO2\n",
    "        print('Loading CO2 condition Data')\n",
    "        co2_mask_file = participant_file_index[CO2Files.MASK.value]\n",
    "        co2_event_file = participant_file_index[CO2Files.EVENT.value]\n",
    "        co2_eyetracking_file = participant_file_index[CO2Files.EYE.value]\n",
    "        co2_biopac_file = participant_file_index[CO2Files.BIOPAC.value]\n",
    "        co2_biopac_start_unix = participant_file_index[CO2Files.BIOPAC_UNIX_START_TIME.value]\n",
    "\n",
    "        # Load mask data\n",
    "        participant.set_co2_mask_data(DataHandler.load_mask_data(co2_mask_file, co2_event_file, participant.id))\n",
    "        # Load eye tracking data\n",
    "        participant.set_co2_eye_data(DataHandler.load_eyetracking_data(co2_eyetracking_file, participant.id, 'co2'))  \n",
    "        # Load biopac data\n",
    "        participant.set_co2_biopac_data(DataHandler.load_biopac_data(co2_biopac_file, co2_biopac_start_unix, participant.id))  \n",
    "        # Sync eye tracking and biopac data\n",
    "        participant.set_co2_synced_data(DataHandler.sync_signal_data(participant.co2_mask_data, participant.co2_eye_data, participant.co2_biopac_data, co2_biopac_start_unix))\n",
    "    \n",
    "        print('Downsampling and combining data')\n",
    "        #Downsample and combine data. This data is also saved\n",
    "        participant.set_synced_data(DataHandler.downsample_participant_data(participant.id, DataHandler.label_data(participant.air_synced_data), DataHandler.label_data(participant.co2_synced_data)))\n",
    "    \n",
    "    # Filter data\n",
    "    filtered_data = DataHandler.filter_data(participant.synced_data)\n",
    "    # Normalise data per participant\n",
    "    normalised_data = DataHandler.normalise_data(filtered_data)\n",
    "    \n",
    "    Plots.participant_overview(normalised_data, True)\n",
    "    \n",
    "    # Feature extraction\n",
    "    windowed_features = DataHandler.extract_features(normalised_data)\n",
    "    segment_features = DataHandler.extract_features_entire_condition(normalised_data)\n",
    "\n",
    "    print('Finished data processing for participant: ' + participant.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac55992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge individual participant windowed feature files \n",
    "combined_features_windowed = DataHandler.merge_participant_windowed_feature_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge individual participant segment feature files\n",
    "combined_features_segments = DataHandler.merge_participant_segment_feature_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
