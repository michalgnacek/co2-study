{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, gc, gzip, pickle, json\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys\n",
    "sys.path.append('../') #to see the utils folder\n",
    "from utils.STResNet import Classifier as STResNet\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#set a nice seaborn style\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prinf libraries versions\n",
    "print('numpy version: ', np.__version__)\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('tensorflow version: ', tf.__version__)\n",
    "#python 3.9.15\n",
    "import sklearn\n",
    "print('sklearn version: ', sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp notebook folder if it does not exist\n",
    "TEMP_FOLDER = 'temp/7_end_to_end_train_eval'\n",
    "if(not os.path.exists(TEMP_FOLDER)):\n",
    "    os.mkdir(TEMP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate LSTM model with 2 lstm layers using keras functional API\n",
    "def generate_LSTM (input_shape, output_shape, num_LSTM_layers=2, num_units=64, dropout=0.2, recurrent_dropout=0.2):\n",
    "    clear_session()\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    #add 1d batch normalization\n",
    "\n",
    "    x = inputs\n",
    "    for i in range(num_LSTM_layers):\n",
    "        x = tf.keras.layers.LSTM(num_units, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout)(x)\n",
    "    x = tf.keras.layers.LSTM(num_units, dropout=dropout, recurrent_dropout=recurrent_dropout)(x)\n",
    "    x = tf.keras.layers.Dense(num_units, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(output_shape, activation='softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#generate Convolutional model with 3 CNN and one fully conencted using keras functional API\n",
    "def generate_CNN (input_shape, output_shape, num_CNN_layers=3, num_units=64, dropout=0.2):\n",
    "    clear_session()\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for i in range(num_CNN_layers):\n",
    "        x = tf.keras.layers.Conv1D(filters=num_units, kernel_size=3, padding='same', activation='relu')(x)\n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(num_units, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(output_shape, activation='softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#generate ConvLSTM mode with 3 CNN layers and 1 LSTM layers using keras functional API\n",
    "def generate_ConvLSTM (input_shape, output_shape, num_CNN_layers=3, num_units=64, dropout=0.2, recurrent_dropout=0.2):\n",
    "    clear_session()\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for i in range(num_CNN_layers):\n",
    "        x = tf.keras.layers.Conv1D(filters=num_units, kernel_size=3, padding='same', activation='relu')(x)\n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.LSTM(num_units, dropout=dropout, recurrent_dropout=recurrent_dropout)(x)\n",
    "    x = tf.keras.layers.Dense(num_units, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(output_shape, activation='softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#generate transofrmer model with 3 layers for a multi channel input using keras functional API\n",
    "def generate_transformer (input_shape, output_shape, num_heads=2, num_transformer_blocks=2, num_units=16, dropout=0.2):\n",
    "    clear_session()\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for i in range(num_transformer_blocks):\n",
    "        x = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=num_units, dropout=dropout)(x, x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        x = tf.keras.layers.Dense(num_units, activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = tf.keras.layers.Dense(output_shape, activation='softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#generate STResnet model\n",
    "def generate_STResnet(signal_size,num_channels,sampling_rate,num_classes):\n",
    "    clear_session()\n",
    "\n",
    "    signal_sizes = [signal_size]*num_channels #in this case the signal size is the same for all channels, but the model aloows different sizes\n",
    "    sampling_rates = [sampling_rate]*num_channels #in this case the sampling rate is the same for all channels, but the model aloows different sizes\n",
    "    fft_win_len_factor = 30 #len of fft windows in seconds\n",
    "    fft_win_length = []\n",
    "    hop_length = [] \n",
    "\n",
    "    for i in range(len(signal_sizes)):\n",
    "            fft_win_length.append(sampling_rates[i]*fft_win_len_factor)\n",
    "            hop_length.append(int(fft_win_length[i]//2))\n",
    "\n",
    "    model_config = []\n",
    "    with open(\"../utils/STResNet_config.json\") as json_file: \n",
    "            model_config = json.load(json_file)\n",
    "            model_config['signal_size'] = signal_sizes\n",
    "            model_config['num_channels'] = len(signal_sizes) \n",
    "            model_config['sampling_rate'] = sampling_rates\n",
    "            \n",
    "            model_config['fft_win_length'] = fft_win_length\n",
    "            model_config['hop_length'] = hop_length\n",
    "            model_config['max_filters'] = 32\n",
    "            model_config['num_filters'] = 16\n",
    "            model_config['num_res_blocks'] = 4\n",
    "\n",
    "    STResNet_model = STResNet(model_config,num_classes,verbose = 1)\n",
    "    \n",
    "    return STResNet_model.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 10\n",
    "\n",
    "INFO_COLUMNS = ['Participant_No', 'Condition', 'Segment','Time']\n",
    "EMG_AMP_COLUMNS = ['Emg/Amplitude[RightOrbicularis]',\n",
    "               'Emg/Amplitude[RightZygomaticus]',\n",
    "               'Emg/Amplitude[RightFrontalis]',\n",
    "               'Emg/Amplitude[CenterCorrugator]',\n",
    "               'Emg/Amplitude[LeftFrontalis]',\n",
    "               'Emg/Amplitude[LeftZygomaticus]',\n",
    "               'Emg/Amplitude[LeftOrbicularis]']\n",
    "EMG_CONTACT_COLUMNS = ['Emg/Contact[RightOrbicularis]',\n",
    "               'Emg/Contact[RightZygomaticus]',\n",
    "               'Emg/Contact[RightFrontalis]',\n",
    "               'Emg/Contact[CenterCorrugator]',\n",
    "               'Emg/Contact[LeftFrontalis]',\n",
    "               'Emg/Contact[LeftZygomaticus]',\n",
    "               'Emg/Contact[LeftOrbicularis]']\n",
    "HR_COLUMNS = ['HeartRate/Average', 'Ppg/Raw.ppg']\n",
    "IMU_COLUMNS = ['Accelerometer/Raw.x', 'Accelerometer/Raw.y', 'Accelerometer/Raw.z', \n",
    "               'Gyroscope/Raw.x', 'Gyroscope/Raw.y', 'Gyroscope/Raw.z']\n",
    "EYE_COLUMNS = ['VerboseData.Right.PupilDiameterMm','VerboseData.Left.PupilDiameterMm']\n",
    "BIOPAC_RR_COLUMNS = ['Biopac_RSP']\n",
    "BIOPAC_GSR_COLUMNS = ['Biopac_GSR']\n",
    "SENSOR_COLUMNS = EMG_AMP_COLUMNS + EMG_CONTACT_COLUMNS + HR_COLUMNS + IMU_COLUMNS + EYE_COLUMNS + BIOPAC_RR_COLUMNS + BIOPAC_GSR_COLUMNS\n",
    "\n",
    "ALL_COLUMNS = INFO_COLUMNS + SENSOR_COLUMNS\n",
    "\n",
    "PROCESSED_DATA_FOLDER = 'temp/6_end_to_end_preprocess_data'\n",
    "\n",
    "\n",
    "#load segments and labels\n",
    "#with gzip.open(os.path.join(PROCESSED_DATA_FOLDER, 'segments_all_sensor_dict.pkl.gz'), 'rb') as f:\n",
    "#    segments_sensor_dict = pickle.load(f)\n",
    "#with gzip.open(os.path.join(PROCESSED_DATA_FOLDER, 'segments_hr_dict.pkl.gz'), 'rb') as f:\n",
    "#    segments_sensor_dict = pickle.load(f)\n",
    "with gzip.open(os.path.join(PROCESSED_DATA_FOLDER, 'segments_labels_dict.pkl.gz'), 'rb') as f:\n",
    "    segments_labels_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_DEVICE = \"0\"\n",
    "def my_set_session():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = TF_DEVICE\n",
    "    physical_devices = tf.config.list_physical_devices('GPU') \n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True) #allocte memory as needed\n",
    "        memory_growth = tf.config.experimental.get_memory_growth(gpu)\n",
    "        print(f\"Memory growth for GPU {gpu.name}: {memory_growth}\")\n",
    "    return()\n",
    "    \n",
    "my_set_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot model shapes \n",
    "model = generate_LSTM(input_shape=(100, 3), output_shape=2)\n",
    "#print only model parameters\n",
    "print('Number of parameters: ', model.count_params())\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot model shapes \n",
    "model = generate_CNN(input_shape=(100, 3), output_shape=2)\n",
    "print('Number of parameters: ', model.count_params())\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot model shapes \n",
    "model = generate_ConvLSTM(input_shape=(100, 3), output_shape=2)\n",
    "print('Number of parameters: ', model.count_params())\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot model shapes \n",
    "model = generate_transformer(input_shape=(100, 3), output_shape=2)\n",
    "print('Number of parameters: ', model.count_params())\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot STResNet shapes \n",
    "model = generate_STResnet(signal_size=100,num_channels=3,sampling_rate=10,num_classes=2)\n",
    "print('Number of parameters: ', model.count_params())\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_DL_model(X_train,y_train,\n",
    "                            X_valid,y_valid,\n",
    "                            X_test,y_test,\n",
    "                            model_func,model_name):\n",
    "    batch_size = 512\n",
    "    epochs = 150\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    output_shape = num_classes\n",
    "\n",
    "  \n",
    "    metric = 'val_accuracy' #STResNet names val_acc, others val_accuracy\n",
    "    if model_name == 'STResNet':\n",
    "        model = model_func(signal_size = X_train[0].shape[-1],\n",
    "                           num_channels = len(X_train),\n",
    "                           sampling_rate = SAMPLING_RATE,\n",
    "                           num_classes=num_classes)\n",
    "        metric = 'val_acc'\n",
    "    else:\n",
    "        input_shape = X_train.shape[1:]\n",
    "        model = model_func(input_shape, output_shape)\n",
    "    \n",
    "  #define callbacks for model checkpointing \n",
    "    model_path = './models/'+model_name+'_model.h5'\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=model_path,\n",
    "                                                                     monitor=metric,\n",
    "                                                                        mode='max',\n",
    "                                                                        verbose=0,\n",
    "                                                                        save_best_only=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, \n",
    "                             epochs=epochs, \n",
    "                             validation_data=(X_valid, y_valid),\n",
    "                             batch_size=batch_size,\n",
    "                                callbacks=[model_checkpoint_callback],\n",
    "                             verbose=0, shuffle=True)\n",
    "    #load best model\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    #evaluate model\n",
    "    y_pred_onehot = model.predict(X_test,verbose=0)\n",
    "    y_pred = np.argmax(y_pred_onehot, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    test_acc = accuracy_score(y_test, y_pred).round(2)\n",
    "\n",
    "    #validation accuracy\n",
    "    y_pred_onehot = model.predict(X_valid,verbose=0)\n",
    "    y_pred = np.argmax(y_pred_onehot, axis=1)\n",
    "    y_valid = np.argmax(y_valid, axis=1)\n",
    "    valid_acc = accuracy_score(y_valid, y_pred).round(2)\n",
    "    print(datetime.now().strftime(\"%H:%M:%S\"), 'Test acc:',test_acc, 'Valid acc:',valid_acc)\n",
    "\n",
    "    del model\n",
    "\n",
    "    return test_acc,y_pred_onehot, history.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColumnIndexRange(columnList):\n",
    "    sensorIndexList = []\n",
    "    for signalColumn in columnList:\n",
    "        sensorIndexList.append(SENSOR_COLUMNS.index(signalColumn))\n",
    "    return sensorIndexList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modalities = {\n",
    "#    'EMG_A': EMG_AMP_COLUMNS,\n",
    "#    'EMG_C': EMG_CONTACT_COLUMNS , \n",
    "#    'HR': HR_COLUMNS,\n",
    "#    'IMU': IMU_COLUMNS ,\n",
    "#    'PupilSize': EYE_COLUMNS ,\n",
    "#    'GSR': BIOPAC_GSR_COLUMNS ,\n",
    "#    'RSP': BIOPAC_RR_COLUMNS ,\n",
    "#    'ALL': SENSOR_COLUMNS \n",
    "#}\n",
    "\n",
    "modalities = {\n",
    "    'EMG_A': 'segments_emg_amp_dict.pkl.gz',\n",
    "    'EMG_C': 'segments_emg_contact_dict.pkl.gz' , \n",
    "    'HR': 'segments_hr_dict.pkl.gz',\n",
    "    'IMU': 'segments_imu_dict.pkl.gz' ,\n",
    "    'PupilSize': 'segments_eye_dict.pkl.gz' ,\n",
    "    'GSR': 'segments_gsr_dict.pkl.gz' ,\n",
    "    'RSP': 'segments_rr_dict.pkl.gz' ,\n",
    "    'ALL': 'segments_all_sensor_dict.pkl.gz' \n",
    "}\n",
    "\n",
    "#functions thatinitiate models\n",
    "model_functions = {'STResNet': generate_STResnet,\n",
    "                   'CNN': generate_CNN, \n",
    "    'ConvLSTM': generate_ConvLSTM,\n",
    "     'LSTM': generate_LSTM,\n",
    "    'Transformer': generate_transformer,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main ML training loop\n",
    "for modality in modalities:\n",
    "    # if modality in ['EMG_A', 'EMG_C']:\n",
    "    #     continue\n",
    "    print('Running for: ' + modality)\n",
    "    #get index of desired signal columns\n",
    "\n",
    "    data_file = os.path.join(PROCESSED_DATA_FOLDER, modalities[modality])\n",
    "    with gzip.open(data_file, 'rb') as f:\n",
    "        print('Loading file: ' + data_file)\n",
    "        segments_sensor_dict = pickle.load(f)\n",
    "\n",
    "    #create 5 folds of non-overalpping user ids for person-independent 5-fold cross validation\n",
    "    users = list(segments_sensor_dict.keys())\n",
    "    np.random.shuffle(users)\n",
    "    folds = np.array_split(users, 5)\n",
    "\n",
    "\n",
    "\n",
    "    test_acc_dict = {}\n",
    "    history_dict = {}\n",
    "    users = list(segments_sensor_dict.keys())\n",
    "    for model_name, model_func in model_functions.items(): \n",
    "        if model_name == 'LSTM':\n",
    "            continue\n",
    "        print('-------model: ', model_name)\n",
    "        test_acc_arr = []\n",
    "        test_predictions_arr = []\n",
    "        train_history_arr = []\n",
    "\n",
    "        for test_users in folds:\n",
    "            print('test users: ', test_users)\n",
    "            #split data into train and test\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            X_valid = []\n",
    "            y_valid = []\n",
    "            X_test = []\n",
    "            y_test = []\n",
    "\n",
    "            validation_counter = 0\n",
    "            num_validation_users = 5\n",
    "            for user in users:\n",
    "                if user in test_users:\n",
    "                    X_test.extend(np.array(segments_sensor_dict[user])) #select only columns we want to use as input\n",
    "                    y_test.extend(segments_labels_dict[user]) \n",
    "                else:\n",
    "                    validation_counter += 1\n",
    "                    if validation_counter <= num_validation_users: #use first n users for validations\n",
    "                        X_valid.extend(np.array(segments_sensor_dict[user]))\n",
    "                        y_valid.extend(segments_labels_dict[user])\n",
    "                    else:\n",
    "                        X_train.extend(np.array(segments_sensor_dict[user])) #select only columns we want to use as input\n",
    "                        y_train.extend(segments_labels_dict[user])\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            X_valid = np.array(X_valid)\n",
    "            y_valid = np.array(y_valid)\n",
    "            X_test = np.array(X_test)\n",
    "            y_test = np.array(y_test)\n",
    "\n",
    "            y_train = y_train[:,0] #the whole segment has the same label, so we can just take the first one\n",
    "            y_valid = y_valid[:,0] #the whole segment has the same label, so we can just take the first one\n",
    "            y_test = y_test[:,0] #the whole segment has the same label, so we can just take the first one\n",
    "\n",
    "            #one-hot encode labels\n",
    "            onehot_encoder = OneHotEncoder(sparse=False)\n",
    "            y_train = onehot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "            y_valid = onehot_encoder.transform(y_valid.reshape(-1, 1))\n",
    "            y_test = onehot_encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "            if model_name == 'STResNet':\n",
    "                X_train = X_train.transpose(2,0,1)\n",
    "                X_valid = X_valid.transpose(2,0,1)\n",
    "                X_test = X_test.transpose(2,0,1)\n",
    "\n",
    "                #convert data into a list of inputs suitable for STResNet\n",
    "                X_train = [x for x in X_train]\n",
    "                X_valid = [x for x in X_valid]\n",
    "                X_test = [x for x in X_test]\n",
    "\n",
    "            if test_users[0] == folds[0][0]: #print only once\n",
    "\n",
    "                print('X_train shape: ', np.array(X_train).shape)\n",
    "                print('X_valid shape: ', np.array(X_valid).shape)\n",
    "                print('X_test shape: ', np.array(X_test).shape)\n",
    "                print('y_train shape: ', y_train.shape)\n",
    "                print('y_valid shape: ', y_valid.shape)\n",
    "                print('y_test shape: ', y_test.shape)\n",
    "                print()\n",
    "\n",
    "            acc, test_predictions, history = train_evaluate_DL_model(X_train,y_train,\n",
    "                                                                        X_valid,y_valid,\n",
    "                                                                     X_test,y_test,\n",
    "                                                                     model_func,model_name=model_name)\n",
    "            clear_session()\n",
    "            gc.collect()\n",
    "            print()\n",
    "\n",
    "            test_acc_arr.append(acc)\n",
    "            test_predictions_arr.append(test_predictions)\n",
    "            train_history_arr.append(history)\n",
    "\n",
    "            del X_train, y_train, X_test, y_test, test_predictions, history\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "        #save results\n",
    "        test_acc_dict[model_name] = test_acc_arr\n",
    "        history_dict[model_name] = train_history_arr\n",
    "\n",
    "        #save intermediate results            \n",
    "        with open(os.path.join(TEMP_FOLDER,modality + '_test_acc_dict.pkl'), 'wb') as f:\n",
    "            pickle.dump(test_acc_dict, f)    \n",
    "        with open(os.path.join(TEMP_FOLDER,modality + '_test_predictions_arr.pkl'), 'wb') as f:\n",
    "            pickle.dump(test_predictions_arr, f)\n",
    "        with open(os.path.join(TEMP_FOLDER,modality + '_history_dict.pkl'), 'wb') as f:\n",
    "            pickle.dump(history_dict, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read intermediate results\n",
    "with open('test_acc_dict.pkl', 'rb') as f:\n",
    "    test_acc_dict = pickle.load(f)\n",
    "with open('history_dict.pkl', 'rb') as f:\n",
    "    history_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a table with results per model with average and standard deviation\n",
    "results_df = pd.DataFrame()\n",
    "for model_name, test_acc_arr in test_acc_dict.items():\n",
    "    results_df.loc[model_name, 'Test acc mean'] = np.mean(test_acc_arr)\n",
    "    results_df.loc[model_name, 'test acc std'] = np.std(test_acc_arr)\n",
    "results_df = results_df.round(2)\n",
    "results_df.to_csv('results_5_fold_user_independent.csv')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt average accuracy withper epoch with\n",
    "model_colors = {'CNN': 'blue', 'STResNet': 'red', 'ConvLSTM': 'green', 'LSTM': 'orange', 'Transformer': 'purple'}\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Training accuracy per epoch')\n",
    "for model_name, history_arr in history_dict.items():\n",
    "    model_acc_arr = []\n",
    "    for history in history_arr:\n",
    "        if model_name == 'STResNet':\n",
    "            fold_acc = history['acc']\n",
    "        else:\n",
    "            fold_acc = history['accuracy']\n",
    "        model_acc_arr.append(fold_acc)\n",
    "    \n",
    "    plt.plot(np.mean(model_acc_arr, axis=0), color=model_colors[model_name], label=model_name)\n",
    "    #plot shaded confidence\n",
    "    mean = np.mean(model_acc_arr, axis=0)\n",
    "    std = np.std(model_acc_arr, axis=0)\n",
    "    plt.fill_between(np.arange(len(mean)), mean-std, mean+std, alpha=0.2, color=model_colors[model_name])\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt average accuracy withper epoch with\n",
    "model_colors = {'CNN': 'blue', 'STResNet': 'red', 'ConvLSTM': 'green', 'LSTM': 'orange', 'Transformer': 'purple'}\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Validation accuracy per epoch')\n",
    "for model_name, history_arr in history_dict.items():\n",
    "    model_acc_arr = []\n",
    "    for history in history_arr:\n",
    "        if model_name == 'STResNet':\n",
    "            fold_acc = history['val_acc']\n",
    "        else:\n",
    "            fold_acc = history['val_accuracy']\n",
    "        model_acc_arr.append(fold_acc)\n",
    "    \n",
    "    plt.plot(np.mean(model_acc_arr, axis=0), color=model_colors[model_name], label=model_name)\n",
    "    #plot shaded confidence\n",
    "    mean = np.mean(model_acc_arr, axis=0)\n",
    "    std = np.std(model_acc_arr, axis=0)\n",
    "    plt.fill_between(np.arange(len(mean)), mean-std, mean+std, alpha=0.2, color=model_colors[model_name])\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "75361f0e1f8a506065144a72338c7da8e1f94863d0b8065db4f832f8a12cf48a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
